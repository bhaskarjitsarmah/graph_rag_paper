{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c56e6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_KEY\")\n",
    "os.environ['LANGCHAIN_PROJECT'] = \"earning_call_eval_v3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab053ae-f7a5-476a-be30-b590d3f4484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "from datasets import Dataset\n",
    "from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall\n",
    "import pandas as pd\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.2)\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\")\n",
    "\n",
    "llm = LangchainLLMWrapper(langchain_llm=llm)\n",
    "embeddings = LangchainEmbeddingsWrapper(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fee3b485-3ace-466c-92fc-2a5347cebaf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Adani Enterprises Ltd.json',\n",
       " 'Adani Ports and Special Economic Zone Ltd.json',\n",
       " 'Apollo Hospitals.json',\n",
       " 'Asian Paints.json',\n",
       " 'Axis Bank.json']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "eval_dir = \"eval_data_json/\"\n",
    "filenames = os.listdir(eval_dir)\n",
    "filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06e50ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(data_dict, num_partitions=3):\n",
    "    # Get the length of the questions list\n",
    "    total_length = len(data_dict[\"question\"])\n",
    "    print(total_length)\n",
    "    \n",
    "    # Calculate the size of each partition\n",
    "    partition_size = total_length // num_partitions\n",
    "    \n",
    "    # Create a list to store the mini-batches\n",
    "    mini_batches = []\n",
    "    \n",
    "    # Iterate through the data in partitions\n",
    "    for i in range(num_partitions):\n",
    "        start_idx = i * partition_size\n",
    "        # Ensure the last partition includes any remaining elements\n",
    "        if i == num_partitions - 1:\n",
    "            end_idx = total_length\n",
    "        else:\n",
    "            end_idx = (i + 1) * partition_size\n",
    "        \n",
    "        # Create a mini-batch dictionary\n",
    "        mini_batch = {\n",
    "            \"question\": data_dict[\"question\"][start_idx:end_idx],\n",
    "            \"answer\": data_dict[\"answer\"][start_idx:end_idx],\n",
    "            \"contexts\": data_dict[\"contexts\"][start_idx:end_idx],\n",
    "            \"ground_truth\": data_dict[\"ground_truth\"][start_idx:end_idx]\n",
    "        }\n",
    "        \n",
    "        # Append the mini-batch to the list\n",
    "        print(start_idx, end_idx)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cc8cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "0 2\n",
      "2 4\n",
      "4 6\n",
      "6 8\n",
      "8 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/8 [01:50<?, ?it/s]\n",
      "Exception in thread Thread-133:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\executor.py\", line 95, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\asyncio\\base_events.py\", line 649, in run_until_complete\n",
      "    return future.result()\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\executor.py\", line 83, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\executor.py\", line 78, in _aresults\n",
      "    r = await future\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\asyncio\\tasks.py\", line 571, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\executor.py\", line 37, in sema_coro\n",
      "    return await coro\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\executor.py\", line 111, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 125, in ascore\n",
      "    raise e\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\metrics\\base.py\", line 121, in ascore\n",
      "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\metrics\\_context_precision.py\", line 161, in _ascore\n",
      "    results = await self.llm.generate(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 93, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 185, in async_wrapped\n",
      "    return await fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\__init__.py\", line 412, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\concurrent\\futures\\_base.py\", line 451, in result\n",
      "    return self.__get_result()\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\concurrent\\futures\\_base.py\", line 403, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\llms\\base.py\", line 170, in agenerate_text\n",
      "    return await self.langchain_llm.agenerate_prompt(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 570, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 530, in agenerate\n",
      "    raise exceptions[0]\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 715, in _agenerate_with_cache\n",
      "    result = await self._agenerate(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\langchain_openai\\chat_models\\base.py\", line 649, in _agenerate\n",
      "    response = await self.async_client.create(messages=message_dicts, **params)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 1283, in create\n",
      "    return await self._post(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1805, in post\n",
      "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1503, in request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1584, in _request\n",
      "    return await self._retry_request(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1630, in _retry_request\n",
      "    return await self._request(\n",
      "  File \"c:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\openai\\_base_client.py\", line 1599, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mExceptionInRunner\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mini_batch \u001b[38;5;129;01min\u001b[39;00m mini_batches:\n\u001b[0;32m     28\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(mini_batch)\n\u001b[1;32m---> 29\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mfaithfulness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43manswer_relevancy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_precision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext_recall\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     df \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[0;32m     31\u001b[0m     list_df\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\evaluation.py:250\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluation_group_cm\u001b[38;5;241m.\u001b[39mended:\n\u001b[0;32m    248\u001b[0m         evaluation_rm\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    252\u001b[0m     result \u001b[38;5;241m=\u001b[39m Result(\n\u001b[0;32m    253\u001b[0m         scores\u001b[38;5;241m=\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_list(scores),\n\u001b[0;32m    254\u001b[0m         dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m    255\u001b[0m         binary_columns\u001b[38;5;241m=\u001b[39mbinary_metrics,\n\u001b[0;32m    256\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\langchain_env\\lib\\site-packages\\ragas\\evaluation.py:232\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(dataset, metrics, llm, embeddings, callbacks, in_ci, is_async, run_config, raise_exceptions, column_map)\u001b[0m\n\u001b[0;32m    230\u001b[0m results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mresults()\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m results \u001b[38;5;241m==\u001b[39m []:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# convert results to dataset_like\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n",
      "\u001b[1;31mExceptionInRunner\u001b[0m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for filename in filenames[11:21]:\n",
    "    i = 0\n",
    "    with open(eval_dir + filename, 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    answers = []\n",
    "    questions = []\n",
    "    contexts = []\n",
    "    ground_truths = []\n",
    "\n",
    "    for item in json_data:\n",
    "        answers.append(item['answer'])\n",
    "        questions.append(item['question'])\n",
    "        contexts.append(item['contexts'])\n",
    "        ground_truths.append(item['ground_truth'])\n",
    "\n",
    "    data_dict = {\n",
    "        \"question\": questions,\n",
    "        \"answer\": answers,\n",
    "        \"contexts\": contexts,\n",
    "        \"ground_truth\": ground_truths\n",
    "    }\n",
    "    mini_batches = create_mini_batches(data_dict, num_partitions=5)\n",
    "    list_df = []\n",
    "    # runing this loop over mini-batches since passing all data at once gives token_per_minute limit excedded error.\n",
    "    for mini_batch in mini_batches:\n",
    "        dataset = Dataset.from_dict(mini_batch)\n",
    "        result = evaluate(dataset, llm=llm, embeddings=embeddings, metrics=[faithfulness, answer_relevancy, context_precision, context_recall])\n",
    "        df = result.to_pandas()\n",
    "        list_df.append(df)\n",
    "        if (i+1) % 2 == 0:\n",
    "            print(f\"SLEEPING at {i+1}\")\n",
    "            time.sleep(60)\n",
    "        i+=1\n",
    "    \n",
    "    concatenated_df = pd.concat(list_df, ignore_index=True)\n",
    "    print(f\"> {filename}, Shape: {concatenated_df.shape}\")\n",
    "    concatenated_df.to_csv(f\"eval_out/{filename.replace('.json', '.csv')}\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b568c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
